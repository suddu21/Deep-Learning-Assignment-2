{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a2d0b2",
   "metadata": {},
   "source": [
    "# DL Assignment 2 - Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfd21b",
   "metadata": {},
   "source": [
    "## Question 1 - Build a simple CNN model\n",
    "* 5 Convolution layers\n",
    "* Each layer followed by Activation and MaxPool\n",
    "* After 5 layers, add a dense layer followed by output layer for 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FlexibleCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_classes=10,\n",
    "                 conv_filters=[32, 32, 32, 32, 32],  # Number of filters for each conv layer\n",
    "                 kernel_size=3,\n",
    "                 activation='relu',\n",
    "                 dense_neurons=64,\n",
    "                 dropout_rate=0.0,\n",
    "                 use_batch_norm=False):\n",
    "        super(FlexibleCNN, self).__init__()\n",
    "        \n",
    "        # Activation function selection\n",
    "        if activation.lower() == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation.lower() == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation.lower() == 'silu':\n",
    "            self.activation = nn.SiLU()\n",
    "        elif activation.lower() == 'mish':\n",
    "            self.activation = nn.Mish()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        in_channels = 3  # RGB input\n",
    "        \n",
    "        for num_filters in conv_filters:\n",
    "            conv_block = []\n",
    "            # Conv layer\n",
    "            conv_block.append(nn.Conv2d(in_channels, num_filters, kernel_size, padding=kernel_size//2))\n",
    "            \n",
    "            # Batch normalization if requested\n",
    "            if use_batch_norm:\n",
    "                conv_block.append(nn.BatchNorm2d(num_filters))\n",
    "            \n",
    "            # Activation\n",
    "            conv_block.append(self.activation)\n",
    "            \n",
    "            # Max pooling\n",
    "            conv_block.append(nn.MaxPool2d(2))\n",
    "            \n",
    "            # Add dropout if rate > 0\n",
    "            if dropout_rate > 0:\n",
    "                conv_block.append(nn.Dropout2d(dropout_rate))\n",
    "            \n",
    "            self.conv_blocks.append(nn.Sequential(*conv_block))\n",
    "            in_channels = num_filters\n",
    "        \n",
    "        # Calculate the size of flattened features\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Dense layers\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(in_channels * 8 * 8, dense_neurons),  # 8x8 due to 5 max pooling layers\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "            nn.Linear(dense_neurons, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply convolutional blocks\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "        \n",
    "        # Flatten and apply dense layers\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c401f",
   "metadata": {},
   "source": [
    "## Question 2 - Train the model with inaturalist_12K dataset\n",
    "* Hyperparameter search is carried out via wandb sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2874433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import wandb\n",
    "#from model import FlexibleCNN\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class iNaturalistDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        try:\n",
    "            self.classes.remove(\".DS_Store\")\n",
    "        except:\n",
    "            pass\n",
    "        #print(self.classes)\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name == \".DS_Store\":\n",
    "                    continue\n",
    "                self.images.append(os.path.join(class_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[class_name])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Set random seeds for reproducibility\n",
    "        torch.manual_seed(config.seed)\n",
    "        random.seed(config.seed)\n",
    "        np.random.seed(config.seed)\n",
    "        \n",
    "        # Data transforms\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            #transforms.RandomRotation(10),\n",
    "            #transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = iNaturalistDataset(r\"/kaggle/input/inaturalist12k/inaturalist_12K/train\", transform=train_transform)\n",
    "        \n",
    "        # Split into train and validation\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        # Apply validation transform to validation dataset\n",
    "        val_dataset.dataset.transform = val_transform\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = FlexibleCNN(\n",
    "            num_classes=10,\n",
    "            conv_filters=config.conv_filters,\n",
    "            activation=config.activation,\n",
    "            dense_neurons=config.dense_neurons,\n",
    "            dropout_rate=config.dropout_rate,\n",
    "            use_batch_norm=config.use_batch_norm\n",
    "        )\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_acc = 0.0\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in tqdm(train_loader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{config.epochs}], \"\n",
    "                  f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "                  f\"Train Acc: {100. * train_correct / train_total:.2f}%, \"\n",
    "                  f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "                  f\"Val Acc: {100. * val_correct / val_total:.2f}%\")\n",
    "            # Log metrics\n",
    "            wandb.log({\n",
    "                'train_loss': train_loss / len(train_loader),\n",
    "                'train_acc': 100. * train_correct / train_total,\n",
    "                'val_loss': val_loss / len(val_loader),\n",
    "                'val_acc': 100. * val_correct / val_total,\n",
    "                'epoch': epoch\n",
    "            })\n",
    "\n",
    "            config = wandb.config\n",
    "            \n",
    "            run_name = \"lr_{}_a_{}_dn_{}_bn_{}_dr{}\".format(\n",
    "                config.learning_rate, config.activation, config.dense_neurons,\n",
    "                config.use_batch_norm, config.dropout_rate\n",
    "            )\n",
    "\n",
    "            wandb.run.name = run_name\n",
    "            wandb.run.save()\n",
    "            \n",
    "            # Save best model\n",
    "            val_acc = 100. * val_correct / val_total\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define sweep configuration\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {\n",
    "            'name': 'val_acc',\n",
    "            'goal': 'maximize'\n",
    "        },\n",
    "        'parameters': {\n",
    "            'seed': {\n",
    "                'values': [42]\n",
    "            },\n",
    "            'batch_size': {\n",
    "                'values': [16]\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'values': [0.001, 0.0001]\n",
    "            },\n",
    "            'epochs': {\n",
    "                'value': 10\n",
    "            },\n",
    "            'conv_filters': {\n",
    "                'values': [\n",
    "                    [32, 32, 32, 32, 32],     # Same filters\n",
    "                    [32, 64, 128, 256, 512],  # Doubling\n",
    "                    [512, 256, 128, 64, 32]   # Halving\n",
    "                ]\n",
    "            },\n",
    "            'activation': {\n",
    "                'values': ['relu', 'gelu', 'silu', 'mish']\n",
    "            },\n",
    "            'dense_neurons': {\n",
    "                'values': [32, 64, 128]\n",
    "            },\n",
    "            'dropout_rate': {\n",
    "                'values': [0.0, 0.2, 0.3]\n",
    "            },\n",
    "            'use_batch_norm': {\n",
    "                'values': [True, False]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"DL_A2\")\n",
    "    wandb.agent(sweep_id, function=train, count=10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
